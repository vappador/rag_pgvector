# .env (or compose env)
OLLAMA_BASE_URL=http://ollama:11434   # if running the ollama container (see below)
EMB_MODEL=mxbai-embed-large           # 1024-dim â†’ matches our current schema
EMB_DIM=1024
LLM_MODEL=llama3.1:8b                 # any local chat model you've pulled
